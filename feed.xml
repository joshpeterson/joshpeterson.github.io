<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joshpeterson.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joshpeterson.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-16T06:16:45+00:00</updated><id>https://joshpeterson.github.io/feed.xml</id><title type="html">blank</title><subtitle>Josh&apos;s personal website </subtitle><entry><title type="html">More fun with loop unrolling - C++</title><link href="https://joshpeterson.github.io/blog/2024/more-fun-with-loop-unrolling-cpp/" rel="alternate" type="text/html" title="More fun with loop unrolling - C++"/><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2024/more-fun-with-loop-unrolling-cpp</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2024/more-fun-with-loop-unrolling-cpp/"><![CDATA[<p>After exploring <a href="/learning-loop-unrolling">loop unrolling in Mojo</a>, I wanted to take a similar path with C++. Can we get similar assembly code from a C++ compiler, given similar input code? Let’s dive in and find out!</p> <h2 id="the-setup">The setup</h2> <p>I’ve written a simple <code class="language-plaintext highlighter-rouge">Matrix</code> class in C++, templated on the row and column size (so those are available at compile time). You can find all of the code for it on <a href="https://github.com/joshpeterson/on-a-roll/tree/main/cpp">Github</a>. Here is the naive <code class="language-plaintext highlighter-rouge">matMul</code> implementation.</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">&gt;</span>
<span class="n">Matrix</span><span class="o">&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="o">&gt;</span> <span class="n">matMul</span><span class="p">(</span><span class="k">const</span> <span class="n">Matrix</span><span class="o">&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="o">&gt;&amp;</span> <span class="n">left</span><span class="p">,</span> <span class="k">const</span> <span class="n">Matrix</span><span class="o">&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="o">&gt;&amp;</span> <span class="n">right</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Matrix</span><span class="o">&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">left</span><span class="p">.</span><span class="n">numberOfRows</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">.</span><span class="n">numberOfColumns</span><span class="p">();</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">result</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">left</span><span class="p">.</span><span class="n">numberOfColumns</span><span class="p">();</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">+=</span> <span class="n">left</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">right</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>I’m testing this using Apple clang version 15 on a macOS M2 processor. As in the previous post, I’ll use Ghidra for local assembly code analysis and Hyperfine for profiling. You can fiddle with the code and compiler options from this post on <a href="https://godbolt.org/z/oTqWWe39z">Compiler Explorer</a>.</p> <p>The naive implementation, with no loop unrolling directives causes clang to produce assembly code for the inner loop similar to what we saw from Mojo.</p> <pre><code class="language-asm">.LBB0_5:
ldr s1, [x9, x14]
add x14, x14, #4
ldr s2, [x16]
add x16, x16, #1, lsl #12
cmp x14, #1, lsl #12
fmadd s0, s1, s2, s0
str s0, [x0, x15, lsl #2]
b.ne .LBB0_5
</code></pre> <p>The heart of this code is the <a href="https://developer.arm.com/documentation/ddi0602/2023-12/SIMD-FP-Instructions/FMADD--Floating-point-fused-Multiply-Add--scalar--?lang=en"><code class="language-plaintext highlighter-rouge">fmadd</code></a> instruction, operating on one 32-bit floating point value at a time. Here is the baseline profiling result for this case:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./build/matmul                                                        
  Time (mean ± σ):      1.267 s ±  0.003 s    [User: 1.243 s, System: 0.003
  Range (min … max):    1.263 s …  1.274 s    10
</code></pre></div></div> <h2 id="lets-unroll-this">Let’s unroll this</h2> <p>With the Mojo case, we found that unrolling the loop gave the compiler the ability to vectorize the loop, and take advantage of the <a href="https://developer.arm.com/documentation/ddi0602/2022-06/SVE-Instructions/FMLA--vectors---Floating-point-fused-multiply-add-vectors--predicated---writing-addend--Zda---Zda---Zn---Zm--"><code class="language-plaintext highlighter-rouge">fmla</code></a> instruction, to give us almost 4 times better performance. We can state our intentions to force loop unrolling to the compiler with the <code class="language-plaintext highlighter-rouge">#pragma unroll(n)</code> directive:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#pragma unroll(4)
</span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">left</span><span class="p">.</span><span class="n">numberOfColumns</span><span class="p">();</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">+=</span> <span class="n">left</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">right</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
</code></pre></div></div> <p>Interestingly, clang gives us four scalar <code class="language-plaintext highlighter-rouge">fmadd</code> instructions:</p> <pre><code class="language-asm">.LBB0_5:
add x17, x10, x14
ldr s1, [x16]
add x14, x14, #16
cmp x14, #1, lsl #12
ldp s2, s3, [x17, #-12]
fmadd s0, s2, s1, s0
ldr s1, [x16, #4096]
ldr s2, [x16, #8192]
fmadd s0, s3, s1, s0
ldur s1, [x17, #-4]
fmadd s0, s1, s2, s0
ldr s1, [x17]
ldr s2, [x16, #12288]
add x16, x16, #4, lsl #12
fmadd s0, s1, s2, s0
str s0, [x0, x15, lsl #2]
b.ne .LBB0_5
</code></pre> <p>The performance is slightly better, but nothing like the 3-4x improvement which is possible:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./build/matmul
  Time (mean ± σ):      1.181 s ±  0.009 s    [User: 1.161 s, System: 0.003 s]
  Range (min … max):    1.163 s …  1.195 s    10 runs
</code></pre></div></div> <p>Why isn’t clang able to vectorize this loop? As it turns out, there is a <a href="https://llvm.org/docs/Vectorizers.html#diagnostics">command line option</a> we can pass to clang to have it tell us that answer: <code class="language-plaintext highlighter-rouge">-Rpass-analysis=loop-vectorize</code>!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>remark: loop not vectorized: cannot prove it is safe to reorder floating-point operations; allow reordering by specifying '#pragma clang loop vectorize(enable)' before the loop or by providing the compiler option '-ffast-math'. [-Rpass-analysis=loop-vectorize]

        result(i, j) += left(i, k) * right(k, j);
</code></pre></div></div> <p>Ahh, the compiler is providing some strict floating point ordering guarantees, and so does not vectorize this loop. If we use the <code class="language-plaintext highlighter-rouge">#pragma clang loop vectorize(enable)</code>, let’s see what happens to the generated assembly code.</p> <pre><code class="language-asm">.LBB0_9:
ldr s4, [x5, x13]
add x6, x5, #1, lsl #12
ldr s3, [x5]
add x7, x5, #5, lsl #12
add x19, x12, x4
add x4, x4, #32
cmp x4, #1, lsl #12
ld1 { v3.s }[1], [x6]
add x6, x5, #2, lsl #12
ld1 { v4.s }[1], [x7]
add x7, x5, #6, lsl #12
ldp q5, q6, [x19, #-16]
ld1 { v3.s }[2], [x6]
add x6, x5, #3, lsl #12
ld1 { v4.s }[2], [x7]
add x7, x5, #7, lsl #12
add x5, x5, #8, lsl #12
ld1 { v3.s }[3], [x6]
ld1 { v4.s }[3], [x7]
fmla v2.4s, v3.4s, v5.4s
fmla v1.4s, v4.4s, v6.4s
b.ne .LBB0_9
</code></pre> <p>We now have two <code class="language-plaintext highlighter-rouge">fmla</code> instructions, but the loop has a lot of other register shifting operations that will likely hurt performance. Here is what Hyperfine has to show us:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./build/matmul
  Time (mean ± σ):      1.002 s ±  0.020 s    [User: 0.978 s, System: 0.003 s]
  Range (min … max):    0.979 s …  1.041 s    10 runs
</code></pre></div></div> <h2 id="lets-try-another-compiler">Let’s try another compiler</h2> <p>One of the great things about the C++ ecosystem is the presence of multiple compiler implementations. Notice in the Compiler Explorer link, I’ve added GCC 13.2. Check out the assembly code it produces for even the naive <code class="language-plaintext highlighter-rouge">matMul</code>, with no unrolling or vectorization directives.</p> <pre><code class="language-asm">.L25:
lsl x8, x3, 12
ldr s2, [x1, x3, lsl 2]
add x3, x3, 1
ldr q1, [x7, x8]
fmla v0.4s, v1.4s, v2.s[0]
cmp x3, 1024
bne .L25
</code></pre> <p>It is vectorizing by default! This looks very similar to the unrolled output we saw from Mojo. Let’s check the performance of this code.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./build/matmul
  Time (mean ± σ):     340.7 ms ±   2.1 ms    [User: 318.0 ms, System: 3.8 ms]
  Range (min … max):   337.7 ms … 344.5 ms    10 runs
</code></pre></div></div> <p>This looks better! We’re at nearly 4 times the performance of the naive implementation compiled with clang. Since GCC is able to “see through” our algorithm, it can vectorize the code and take advantage of the SIMD instructions available on the processor.</p> <h2 id="c-vs-mojo">C++ vs. Mojo</h2> <p>Note that this is not a direct comparison of C++ and Mojo performance. The performance I am measuring involves more than just <code class="language-plaintext highlighter-rouge">matMul</code> (although that algorithm does dominate the execution time). Things like memory management and random floating point generation also differ.</p> <p>It is a comparison of the assembly code that different languages and compilers emit. I find it fascinating to see such similar code generated by two different language implementations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[After exploring loop unrolling in Mojo, I wanted to take a similar path with C++. Can we get similar assembly code from a C++ compiler, given similar input code? Let’s dive in and find out!]]></summary></entry><entry><title type="html">Learning loop unrolling</title><link href="https://joshpeterson.github.io/blog/2024/learning-loop-unrolling/" rel="alternate" type="text/html" title="Learning loop unrolling"/><published>2024-02-12T00:00:00+00:00</published><updated>2024-02-12T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2024/learning-loop-unrolling</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2024/learning-loop-unrolling/"><![CDATA[<p>In the excellent Freakonomics Radio <a href="https://freakonomics.com/podcast-tag/richard-feynman/">podcast series</a> about physicist Richard Feynman (stop reading now an go listen to it!) I heard an interesting tidbit from Feynman:</p> <blockquote> <p>Knowing the name of something doesn’t mean you understand it. (<a href="https://www.youtube.com/watch?v=px_4TxC2mXU">video</a>)</p> </blockquote> <p>After reading a recent blog post from Modular about <a href="https://www.modular.com/blog/what-is-loop-unrolling-how-you-can-speed-up-mojo">loop unrolling</a>, I realized that I know what loop unrolling is, but I don’t really understand it. This post is my journey to a deeper understanding.</p> <h2 id="the-setup">The setup</h2> <p>I decided to start this deep dive with the <a href="https://docs.modular.com/mojo/notebooks/Matmul.html">matrix multiplication documentation</a> for Mojo, to see how loop unrolling impacts the naive “matmul” algorithm.</p> <p>I’m doing all of this on an M2 macOS machine, but the results should apply equally to x64 processors.</p> <p>I’m using a few excellent tools:</p> <ul> <li><a href="https://ghidra-sre.org/">Ghidra</a> to analyze the assembly code built by Mojo</li> <li><a href="https://github.com/sharkdp/hyperfine">Hyperfine</a> for benchmarking</li> <li><a href="https://github.com/mstange/samply">Samply</a> for profiling</li> </ul> <p>I’ll be fiddling with this Mojo code, which depends on the <code class="language-plaintext highlighter-rouge">Matrix</code> type implemented in the documentation linked above, using elements of type <code class="language-plaintext highlighter-rouge">DType.float32</code>. You can find all the code I used for this exploration on <a href="https://github.com/joshpeterson/on-a-roll/tree/main/mojo">Github</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alias</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="n">fn</span> <span class="nf">matmul_naive</span><span class="p">(</span><span class="n">C</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span>
 
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
  <span class="n">var</span> <span class="n">C</span> <span class="o">=</span> <span class="nc">Matrix</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="n">var</span> <span class="n">A</span> <span class="o">=</span> <span class="n">Matrix</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="n">var</span> <span class="n">B</span> <span class="o">=</span> <span class="n">Matrix</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

  <span class="nf">matmul_naive</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
 
  <span class="n">A</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">free</span><span class="p">()</span>
  <span class="n">B</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">free</span><span class="p">()</span>
  <span class="n">C</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">free</span><span class="p">()</span>
</code></pre></div></div> <h2 id="the-wrong-path">The wrong path</h2> <p>Sometimes blog posts make it seem like the author knew what they were doing the whole time. For me at least, this is usually not the case. I’ll start by showing how I went down the wrong path.</p> <p>First, I compiled the code with <code class="language-plaintext highlighter-rouge">mojo build</code>, then used Ghidra to look at the assembly code. Most of the work here is done in this loop (which is the inner-most of the three loops):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LAB_100000c48
ldr        s1,[x15], #0x4
ldr        s2,[x16]
fmadd      s1,s0,s1,s2
str        s1,[x16], #0x4
sub        x17,x17,#0x1
cmp        x17,#0x1
b.hi       LAB_100000c48
</code></pre></div></div> <p>The third instruction - <a href="https://developer.arm.com/documentation/ddi0602/2023-12/SIMD-FP-Instructions/FMADD--Floating-point-fused-Multiply-Add--scalar--?lang=en"><code class="language-plaintext highlighter-rouge">fmadd</code></a> - is the key one here. It is doing one multiply and one add operation, all in one instruction, on 32-bit floating point values. After the mathematics, the loop counter in the <code class="language-plaintext highlighter-rouge">x17</code> register is decremented, and the code jumps back to the top of the loop for the next iteration.</p> <p>So, I wondered what would happen if I added the <code class="language-plaintext highlighter-rouge">unroll</code> directive to the outer most loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fn</span> <span class="nf">matmul_naive</span><span class="p">(</span><span class="n">C</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">):</span>
  <span class="nd">@unroll</span> <span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span>
</code></pre></div></div> <p>The Mojo compiler dutifully gives me four loops, with three additional copies of the assembly code loop shown above plus some setup and teardown code in between (I’ll avoid showing them here to save space). So this is loop unrolling, cool!</p> <p>The code must be faster now, right? Let’s see what Hyperfine tells us. I benchmarked the implementation without unrolling against unrolling the loop 4, 8, 16, and 32 times.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./matmul
	Time (mean ± σ): 361.3 ms ± 3.1 ms [User: 342.3 ms, System: 2.0 ms]
	Range (min … max): 357.3 ms … 368.2 ms 10 runs

Benchmark 2: ./matmul_unrolled4
	Time (mean ± σ): 387.7 ms ± 1.7 ms [User: 369.3 ms, System: 1.9 ms]
	Range (min … max): 384.9 ms … 389.9 ms 10 runs

Benchmark 3: ./matmul_unrolled8
	Time (mean ± σ): 389.7 ms ± 5.2 ms [User: 373.5 ms, System: 1.7 ms]
	Range (min … max): 376.6 ms … 394.3 ms 10 runs

Benchmark 4: ./matmul_unrolled16
	Time (mean ± σ): 394.7 ms ± 1.5 ms [User: 376.3 ms, System: 1.8 ms]
	Range (min … max): 392.4 ms … 396.7 ms 10 runs

Benchmark 5: ./matmul_unrolled32
	Time (mean ± σ): 422.0 ms ± 2.4 ms [User: 403.4 ms, System: 1.9 ms]
	Range (min … max): 416.3 ms … 425.6 ms 10 runs

Summary
./matmul ran
	1.07 ± 0.01 times faster than ./matmul_unrolled4
	1.08 ± 0.02 times faster than ./matmul_unrolled8
	1.09 ± 0.01 times faster than ./matmul_unrolled16
	1.17 ± 0.01 times faster than ./matmul_unrolled32
</code></pre></div></div> <p>Huh? The code was faster before I started to fiddle with loop unrolling. In all cases, I made the code execute more slowly! I was very confused. Had I read to the end of the Mojo matrix multiply documentation, I would have noticed that <code class="language-plaintext highlighter-rouge">unroll</code> makes more sense on the inner loop than the outer loop. But I instead went on a hunt to better understand loop unrolling in other languages.</p> <p>Thankfully, I stumbled across the ARM C++ compiler documentation for <a href="https://developer.arm.com/documentation/dui0491/i/Compiler-specific-Features/-pragma-unroll---n--"><code class="language-plaintext highlighter-rouge">#pragma unroll</code></a>](which is how C++ compilers spell <code class="language-plaintext highlighter-rouge">@unroll</code>). Check out this example:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matrix_multiply</span><span class="p">(</span><span class="kt">float</span> <span class="o">**</span> <span class="kr">__restrict</span> <span class="n">dest</span><span class="p">,</span> <span class="kt">float</span> <span class="o">**</span> <span class="kr">__restrict</span> <span class="n">src1</span><span class="p">,</span> <span class="kt">float</span> <span class="o">**</span> <span class="kr">__restrict</span> <span class="n">src2</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span> 
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">;</span> 
  <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> 
    <span class="k">for</span> <span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> 
      <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
      <span class="cm">/* #pragma unroll */</span> 
      <span class="k">for</span><span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> 
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">src1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">src2</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">];</span> <span class="n">dest</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>It looks very similar to the naive matrix multiply implementation in Mojo. You can fiddle with <a href="https://godbolt.org/z/558hMnYKb">this example</a> on Compiler Explorer to see the impact of loop unrolling in C++. It got me to thinking - maybe I should try to unroll the inner-most loop in Mojo as well.</p> <h2 id="getting-it-right">Getting it right</h2> <p>As the Mojo documentation for matrix multiplication suggests, forcing the inner loop to unroll <em>does</em> improve performance. Let’s get straight to the results from Hyperfine:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Benchmark 1: ./matmul
  Time (mean ± σ):     362.0 ms ±   4.1 ms    [User: 341.3 ms, System: 1.8 ms]
  Range (min … max):   356.0 ms … 371.9 ms    10 runs
 
Benchmark 2: ./matmul_unrolled4
  Time (mean ± σ):     128.2 ms ±   3.9 ms    [User: 108.6 ms, System: 1.6 ms]
  Range (min … max):   124.4 ms … 143.4 ms    21 runs
 
Benchmark 3: ./matmul_unrolled8
  Time (mean ± σ):     116.2 ms ±   4.1 ms    [User: 98.3 ms, System: 1.6 ms]
  Range (min … max):   102.8 ms … 119.4 ms    24 runs
 
Benchmark 4: ./matmul_unrolled16
  Time (mean ± σ):     114.3 ms ±   3.9 ms    [User: 95.3 ms, System: 1.6 ms]
  Range (min … max):   104.1 ms … 125.7 ms    22 runs
 
Benchmark 5: ./matmul_unrolled32
  Time (mean ± σ):     114.3 ms ±   2.9 ms    [User: 95.3 ms, System: 1.6 ms]
  Range (min … max):   109.3 ms … 121.7 ms    23 runs
 
Summary
  ./matmul_unrolled32 ran
    1.00 ± 0.04 times faster than ./matmul_unrolled16
    1.02 ± 0.04 times faster than ./matmul_unrolled8
    1.12 ± 0.04 times faster than ./matmul_unrolled4
    3.17 ± 0.09 times faster than ./matmul
</code></pre></div></div> <p>Now this is cool! The unrolled versions all ran about 3 times faster than the naive implementation. But this is not really a complete understanding. Why is the code so much faster with inner loop unrolled? Let’s have a look at the assembly code for that unrolled inner loop.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LAB_100001724                                   
ldr        q1,[x15], #0x10
ldr        q2,[x16]
fmla       v2.4S,v0.4S,v1.4S
str        q2,[x16], #0x10
sub        x17,x17,#0x4
cmp        x17,#0x4
b.hi       LAB_100001724
</code></pre></div></div> <p>Can you spot the difference? It took me a while, so let’s look at the normal code and the unrolled code side by side.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Normal               Unrolled
================== | ======================
ldr s1,[x15], #0x4 | ldr q1,[x15], #0x10
ldr s2,[x16]       | ldr q2,[x16]
fmadd s1,s0,s1,s2  | fmla v2.4S,v0.4S,v1.4S
str s1,[x16], #0x4 | str q2,[x16], #0x10
sub x17,x17,#0x1   | sub x17,x17,#0x4
cmp x17,#0x1       | cmp x17,#0x4
b.hi LAB_100000c48 | b.hi LAB_100001724
</code></pre></div></div> <p>Check out the multiply and add operation, it is spelled differently - <a href="https://developer.arm.com/documentation/ddi0602/2022-06/SVE-Instructions/FMLA--vectors---Floating-point-fused-multiply-add-vectors--predicated---writing-addend--Zda---Zda---Zn---Zm--"><code class="language-plaintext highlighter-rouge">fmla</code></a>. This an ARM instruction that does floating point multiply and add, but on a <em>vector</em> of four values, rather than a <em>scalar</em> of one value. Also, notice the subtraction in register <code class="language-plaintext highlighter-rouge">x17</code> - it now iterates the loop counter by 4, instead of 1. Each time through the unrolled loop, we get four operations, not just one.</p> <p>We can confirm this by running the Samply profiler:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>samply record ./matmul
</code></pre></div></div> <p><img src="/static/images/learning-loop-unrolling/samply_matmul.png" alt="samply_matmul.png" title="Screen shot of Samply output for matmul, showing 327 calls to decrement the loop counter"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>samply record ./matmul_unrolled4
</code></pre></div></div> <p><img src="/static/images/learning-loop-unrolling/samply_matmul_unrolled4.png" alt="samply_matmul.png" title="Screen shot of Samply output for matmul_unrolled4, showing 92 calls to decrement the loop counter"/></p> <p>While Samply does not report exactly four times fewer calls to the loop decrement, it is pretty close. The unrolled inner loop is doing significantly fewer iterations.</p> <h2 id="compilers-️-processors">Compilers ❤️ Processors</h2> <p>What is really going on here?</p> <p>By telling the Mojo compiler to unroll that inner loop, we’re giving it a better chance to use the available hardware. ARMv8-A processors (like the ones in my Mac) have 32 <a href="https://developer.arm.com/documentation/den0024/a/ARMv8-Registers/NEON-and-floating-point-registers/Scalar-register-sizes">registers</a> that are 128 bits wide. The “S” prefix from the normal assembly code means “Word”, or a 32-bit value. The “Q” prefix from the unrolled assembly code means “Quadword”, or a 128-bit value.</p> <p>In this example our data type is 32-bit floating point values, so the normal example is not using all of the available register space. It is only using 32 bits (one-fourth) of registers <code class="language-plaintext highlighter-rouge">s0</code>, <code class="language-plaintext highlighter-rouge">s1</code>, and <code class="language-plaintext highlighter-rouge">s2</code> during each loop. But with the unrolled loop, the compiler is able to notice the chance to “vectorize” the code, and use the full width of each 128-bit register, where it can put four 32-bit floating point values side by side, and then use the <code class="language-plaintext highlighter-rouge">fmla</code> instruction to compute the multiply and add operation on all four together. The ARM <a href="https://developer.arm.com/documentation/den0024/a/ARMv8-Registers/NEON-and-floating-point-registers/Vector-register-sizes">documentation</a> explains the interesting <code class="language-plaintext highlighter-rouge">.4s</code> syntax on each of the arguments to <code class="language-plaintext highlighter-rouge">fmla</code>.</p> <blockquote> <p>Vn.4S - 4 lanes, each containing a 32-bit element</p> </blockquote> <p>So the compiler is able to more efficiently use the registers available on this process.</p> <p>Why do we need to manually unroll the inner loop to allow the compiler to see this possible optimization? I’m not entirely sure. I suspect the compiler has some limited budget for time spent on loop analysis. After all, we want our to to compile fast as well.</p> <h2 id="where-to-next">Where to next?</h2> <p>This example is certainly not the best possible matrix multiplication algorithm. The Modular documentation linked above goes into much more detail about techniques to improve its performance. But it does give us a fun little insight into loop unrolling though, and specifically into how it is implemented in Mojo. I wonder what this example looks like in other languages.</p> <h2 id="appendix">Appendix</h2> <p>Notice in the benchmarking data for the unrolled inner loop, we get slightly better performance as we unroll that inner loop more and more. Skipping by 8, 16, and eventually 32 iterations each yields small marginal gains - but why?</p> <p>The assembly for the <code class="language-plaintext highlighter-rouge">@unroll(32)</code> case provides a hint:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LAB_1000016dc 
ldp        q1,q2,[x17, #-0x40]
ldp        q3,q4,[x16, #-0x40]
fmla       v3.4S,v0.4S,v1.4S
fmla       v4.4S,v0.4S,v2.4S
stp        q3,q4,[x16, #-0x40]
ldp        q1,q2,[x17, #-0x20]
ldp        q3,q4,[x16, #-0x20]
fmla       v3.4S,v0.4S,v1.4S
fmla       v4.4S,v0.4S,v2.4S
stp        q3,q4,[x16, #-0x20]
ldp        q1,q2,[x17]
ldp        q3,q4,[x16]
fmla       v3.4S,v0.4S,v1.4S
fmla       v4.4S,v0.4S,v2.4S
stp        q3,q4,[x16]
ldp        q1,q2,[x17, #0x20]
ldp        q3,q4,[x16, #0x20]
fmla       v3.4S,v0.4S,v1.4S
fmla       v4.4S,v0.4S,v2.4S
stp        q3,q4,[x16, #0x20]
sub        x0,x0,#0x20
add        x17,x17,#0x80
add        x16,x16,#0x80
cmp        x0,#0x20
b.hi       LAB_1000016dc
</code></pre></div></div> <p>There are eight <code class="language-plaintext highlighter-rouge">fmla</code> instructions, set up in four pairs, where each instruction in a given pair is using different result registers. I’m not sure how the Apple M2 implementation works, but I suspect it can take advantage of hardware-level parallelism here, and exec both of the <code class="language-plaintext highlighter-rouge">fmla</code> instructions in a pair at the same time. It sounds like I have more to explore!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In the excellent Freakonomics Radio podcast series about physicist Richard Feynman (stop reading now an go listen to it!) I heard an interesting tidbit from Feynman:]]></summary></entry><entry><title type="html">Constraints are liberating</title><link href="https://joshpeterson.github.io/blog/2023/constraints-are-liberating/" rel="alternate" type="text/html" title="Constraints are liberating"/><published>2023-08-26T00:00:00+00:00</published><updated>2023-08-26T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2023/constraints-are-liberating</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2023/constraints-are-liberating/"><![CDATA[<p>Often constraints seem problematic. We say, “If only this wasn’t holding me back, then I could succeed.” But is that really true? More often than not, constraints can actually be helpful.</p> <h2 id="why-do-we-play-games">Why do we play games?</h2> <p>Think about games for a moment. What are they but arbitrary sets of constraints? Soccer (aka football) is one of my favorite games. I enjoy playing it, watching it, reading about it. I especially like watching this guy:</p> <p><img src="/static/images/constraints-are-liberating/Lionel Messi.jpg" alt="Lionel Messi playing soccer" title="Licensed under CC BY-SA 3.0 Source: https://commons.wikimedia.org/wiki/File:Lionel_Andr%C3%A9s_Messi_Cuccittini.jpg"/></p> <p>The things he does within the odd constraints of this game are beautiful to watch.</p> <p>Why would humans, who have these amazing hands - capable of so many things - want to avoid using them? And why is this game, where we use only our feet, the most popular game in the world? <strong>Secretly, we love constraints.</strong></p> <h2 id="the-engineering-mindset">The engineering mindset</h2> <p>The reality of our existence is that we are constantly constrained. We are constrained spatially by physics, psychologically by emotions, mentally by exhaustion. But constraints are the fertile soil where problems grow.</p> <p><img src="/static/images/constraints-are-liberating/Industrial_Engineer_Working.jpg" alt="Engineers working" title="Licensed under CC BY-SA 3.0 Source: https://commons.wikimedia.org/wiki/File:Industrial_Engineer_Working.jpg"/></p> <p>What are these two engineers doing? It looks like they are solving a problem. Engineering is the practice of solving problems amid constraints. Aren’t we all engineers sometimes?</p> <h2 id="the-artistic-mindset">The artistic mindset</h2> <p>Sometimes, like with games, we impose constraints upon ourselves to provide some unique outcome.</p> <p><img src="/static/images/constraints-are-liberating/Artist-painting.jpeg" alt="An artist painting a face" title="Licensed under CC BY-SA 3.0 Source: https://commons.wikimedia.org/wiki/File:Artist-painting.jpg"/></p> <p>Maybe this artist is painting a portrait of someone. Wouldn’t it be better to take a photo of that person? Certainly the photo would provide more detail about the person’s face. It would be an imperfect, yet more faithful representation of the truth.</p> <p>There is something about the constraint of pigment on a canvas, something about the creativity that constraint allows which means the final portrait is different, and maybe somehow better, than a precise photo. Aren’t we all artists sometimes?</p> <h2 id="creativity-is-the-key">Creativity is the key</h2> <p>Sometimes constraints are thrust upon us - we need an engineering mindset to solve problems (this is why I love writing software). Sometimes we willfully accept constraints - we use an artistic mindset to fashion something beautiful (like Messi does on the soccer field).</p> <p>In both cases, we use creativity to “escape” the constraints, in some sense. It is this process of creation that truly sets us free, that gives us satisfaction.</p> <p>Since we cannot escape constraints, let’s instead embrace them!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Often constraints seem problematic. We say, “If only this wasn’t holding me back, then I could succeed.” But is that really true? More often than not, constraints can actually be helpful.]]></summary></entry><entry><title type="html">Span - making C arrays fun since 2020</title><link href="https://joshpeterson.github.io/blog/2022/span-making-c-arrays-fun-since-2020/" rel="alternate" type="text/html" title="Span - making C arrays fun since 2020"/><published>2022-01-24T00:00:00+00:00</published><updated>2022-01-24T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2022/span-making-c-arrays-fun-since-2020</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2022/span-making-c-arrays-fun-since-2020/"><![CDATA[<p>I just love <a href="https://en.cppreference.com/w/cpp/container/span"><code class="language-plaintext highlighter-rouge">std::span</code></a>! I’ve written about it before <a href="/using-span-with-argv">here</a> and <a href="/a-zero-cost-abstraction">here</a>. Starting in C++20 our friend <code class="language-plaintext highlighter-rouge">span</code> lets us write expressive code with little or no cost in standard C++.</p> <p>And now I’ve learned that <code class="language-plaintext highlighter-rouge">span</code> can help us make C arrays safe, fun, and expressive too!</p> <h2 id="are-c-arrays-evil">Are C arrays evil?</h2> <p>Let’s face it, C arrays can be difficult to use properly. They don’t carry around information about their size that is available at run time, they decay to pointers (which might be <a href="https://digitalmars.com/articles/C-biggest-mistake.html">C’s biggest mistake</a>), and they are notoriously difficult to get right in large code bases. As they get passed around they can cause subtle security problems.</p> <p>But they are also so simple and expressive, I really want to be able to use them with confidence! That’s where are friend <code class="language-plaintext highlighter-rouge">span</code> comes in.</p> <h2 id="tldr">TL;DR</h2> <p>If you have a function that accepts a <code class="language-plaintext highlighter-rouge">span</code> to express the intent that the function expects a continuous block of a fixed number of objects, passing a C array to that argument might be the best option for code readability, expressiveness, and performance.</p> <p>You can find all of the code discussed in this post on Compiler Explorer <a href="https://godbolt.org/z/67brrqbfa">here</a>.</p> <h2 id="the-setup">The setup</h2> <p>Suppose you have a function like this to process a list of points:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">struct</span> <span class="nc">Point</span> <span class="p">{</span> <span class="kt">int</span> <span class="n">x</span><span class="p">;</span> <span class="kt">int</span> <span class="n">y</span><span class="p">;</span> <span class="p">};</span>

<span class="n">Point</span> <span class="nf">Process</span><span class="p">(</span><span class="n">span</span><span class="o">&lt;</span><span class="n">Point</span><span class="o">&gt;</span> <span class="n">points</span><span class="p">){</span>
  <span class="n">print</span><span class="p">(</span><span class="s">"Processing {} points</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">points</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="n">Point</span> <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">};</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">point</span> <span class="o">:</span> <span class="n">points</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result</span><span class="p">.</span><span class="n">x</span> <span class="o">+=</span> <span class="n">point</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="n">result</span><span class="p">.</span><span class="n">y</span> <span class="o">+=</span> <span class="n">point</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <blockquote> <p>These examples will use the <a href="https://fmt.dev">fmt</a> library to print to standard out.</p> </blockquote> <p>This function sums the x and y values, and returns a <code class="language-plaintext highlighter-rouge">Point</code> containing the sum, although the behavior of the <code class="language-plaintext highlighter-rouge">Process</code> function is not too important - we really care about the functions that call it.</p> <p>How could we feed information into that <code class="language-plaintext highlighter-rouge">Process</code> function? We’ll try three different approaches, and output the results like this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="nf">display</span><span class="p">(</span><span class="n">Point</span> <span class="n">point</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">print</span><span class="p">(</span><span class="s">"Point: ({}, {})</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">point</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">point</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">display</span><span class="p">(</span><span class="n">UseVector</span><span class="p">());</span>
  <span class="n">display</span><span class="p">(</span><span class="n">UseStdArray</span><span class="p">());</span>
  <span class="n">display</span><span class="p">(</span><span class="n">UseCArray</span><span class="p">());</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="first-up---vector">First up - vector</h2> <p>By default, I always reach for <code class="language-plaintext highlighter-rouge">std::vector</code> when I need an container of contiguous objects, or, really, any container. Is the best default option, efficient and flexible. The code to feed our points to <code class="language-plaintext highlighter-rouge">Process</code> looks like this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">Point</span> <span class="nf">UseVector</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">Point</span><span class="o">&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">},</span> <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}};</span> 
  <span class="k">return</span> <span class="nf">Process</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure> <p>Use the Compiler Explorer link above to have a look at the assembly code GCC generates at <code class="language-plaintext highlighter-rouge">-O2</code>:</p> <figure class="highlight"><pre><code class="language-nasm" data-lang="nasm"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td class="code"><pre><span class="nf">UseVector</span><span class="p">():</span>
        <span class="nf">push</span>    <span class="nv">r12</span>
        <span class="nf">mov</span>     <span class="nb">edi</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">8589934593</span>
        <span class="nf">push</span>    <span class="nb">rbp</span>
        <span class="nf">sub</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">72</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">32</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">17179869187</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">40</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">25769803781</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">48</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="mi">0</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">8</span><span class="p">],</span> <span class="mi">0</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="mi">0</span>
        <span class="nf">call</span>    <span class="nv">operator</span> <span class="nv">new</span><span class="p">(</span><span class="nv">unsigned</span> <span class="nv">long</span><span class="p">)</span>
        <span class="nf">mov</span>     <span class="nb">rdx</span><span class="p">,</span> <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">48</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">rbp</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">3</span>
        <span class="nf">movdqa</span>  <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">32</span><span class="p">]</span>
        <span class="nf">lea</span>     <span class="nb">rax</span><span class="p">,</span> <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">24</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">rdi</span><span class="p">,</span> <span class="nb">rbp</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="nb">rdx</span>
        <span class="nf">movups</span>  <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">0</span><span class="p">],</span> <span class="nv">xmm0</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">8</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">call</span>    <span class="nv">Process</span><span class="p">(</span><span class="nv">std</span><span class="p">::</span><span class="nb">sp</span><span class="nv">an</span><span class="o">&lt;</span><span class="nv">Point</span><span class="p">,</span> <span class="mi">18446744073709551615</span><span class="nv">ul</span><span class="o">&gt;</span><span class="p">)</span>
        <span class="nf">mov</span>     <span class="nb">rdi</span><span class="p">,</span> <span class="nb">rbp</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">mov</span>     <span class="nv">r12</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">call</span>    <span class="nv">operator</span> <span class="nv">delete</span><span class="p">(</span><span class="nv">void</span><span class="o">*</span><span class="p">,</span> <span class="nv">unsigned</span> <span class="nv">long</span><span class="p">)</span>
        <span class="nf">add</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">72</span>
        <span class="nf">mov</span>     <span class="nb">rax</span><span class="p">,</span> <span class="nv">r12</span>
        <span class="nf">pop</span>     <span class="nb">rbp</span>
        <span class="nf">pop</span>     <span class="nv">r12</span>
        <span class="nf">ret</span>
        <span class="nf">mov</span>     <span class="nb">rbp</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">jmp</span>     <span class="nv">.L20</span>
        <span class="nf">mov</span>     <span class="nb">rbp</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">jmp</span>     <span class="nv">.L21</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>There is a lot going on here! Recall, we want to put six four-byte integers into memory (or registers) and pass them to the <code class="language-plaintext highlighter-rouge">Process</code> function, along with information that those integers make up three <code class="language-plaintext highlighter-rouge">Point</code> objects - that’s it.</p> <p>And eek! Check out the call to operator <code class="language-plaintext highlighter-rouge">new</code> on line 15 and operator <code class="language-plaintext highlighter-rouge">delete</code> on line 31. The code is allocating memory from the heap for something that is completely determined at compile time.</p> <p>OK, so a vector might not be the best option here. Let’s look at arrays.</p> <h2 id="stdarray-vs-c-array">std::array vs. C array</h2> <p>The C++ code to use both <code class="language-plaintext highlighter-rouge">std::array</code> and a C array looks pretty similar to the code for <code class="language-plaintext highlighter-rouge">std::vector</code>:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">Point</span> <span class="nf">UseStdArray</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">array</span><span class="o">&lt;</span><span class="n">Point</span><span class="p">,</span> <span class="mi">3</span><span class="o">&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="p">{{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">},</span> <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}}};</span>
  <span class="k">return</span> <span class="nf">Process</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">Point</span> <span class="nf">UseCArray</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">Point</span> <span class="n">input</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">},</span> <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}};</span>
  <span class="k">return</span> <span class="nf">Process</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure> <p>It is really cool that GCC generates the same assembly code for both of these functions:</p> <figure class="highlight"><pre><code class="language-nasm" data-lang="nasm"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="nf">UseStdArray</span><span class="p">():</span> <span class="c1">; or, UseCArray():</span>
        <span class="nf">sub</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">40</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">3</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">8589934593</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="nb">rdi</span><span class="p">,</span> <span class="nb">rsp</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">17179869187</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">8</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">25769803781</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">call</span>    <span class="nv">Process</span><span class="p">(</span><span class="nv">std</span><span class="p">::</span><span class="nb">sp</span><span class="nv">an</span><span class="o">&lt;</span><span class="nv">Point</span><span class="p">,</span> <span class="mi">18446744073709551615</span><span class="nv">ul</span><span class="o">&gt;</span><span class="p">)</span>
        <span class="nf">add</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">40</span>
        <span class="nf">ret</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>And the code here looks much simpler. On line 3 we put the size of the array into a register, then the following lines get the values on to the stack, and then make call to <code class="language-plaintext highlighter-rouge">Process</code>. Short of computing the result at compile time (spoiler alert!) this seems like the best we can do.</p> <p>So which is better C++ code - <code class="language-plaintext highlighter-rouge">UseStdArray</code> or <code class="language-plaintext highlighter-rouge">UseCArray</code>?</p> <p>First, check out the extra curly braces required for the <code class="language-plaintext highlighter-rouge">std::array</code> case. It turns out there is some confusion among compilers about how this should work, with <a href="https://stackoverflow.com/questions/8192185/using-stdarray-with-initialization-lists">GCC at least</a> reporting an error when they are not there. They just add unnecessary visual clutter, so I prefer a solution without them.</p> <p>Second, we must indicate the size of the <code class="language-plaintext highlighter-rouge">std::array</code> in the code. In this case, when the array is initialized with data so the information about the size is repeated, and that can lead to problems. It seems better to <a href="https://quuxplusone.github.io/blog/2020/08/06/array-size/">avoid mentioning the size twice</a> at all.</p> <p>The C array wins on both of these points - its initialization is simple as it can be, and its size is inferred from the its initial value - cool! Then of course inside <code class="language-plaintext highlighter-rouge">Process</code> the array becomes a <code class="language-plaintext highlighter-rouge">span</code>, so it is safe to iterate and use without any worry about buffer overflows.</p> <h2 id="but-wait-theres-more">But wait, there’s more</h2> <p>Let’s crank up the optimization level to <code class="language-plaintext highlighter-rouge">-O3</code> and see if that helps the <code class="language-plaintext highlighter-rouge">std::vector</code>’s case here.</p> <figure class="highlight"><pre><code class="language-nasm" data-lang="nasm"><span class="nf">UseVector</span><span class="p">():</span>
        <span class="nf">push</span>    <span class="nv">r12</span>
        <span class="nf">mov</span>     <span class="nb">edi</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">push</span>    <span class="nb">rbp</span>
        <span class="nf">push</span>    <span class="nb">rbx</span>
        <span class="nf">sub</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">32</span>
        <span class="nf">movdqa</span>  <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="nv">.LC1</span><span class="p">[</span><span class="nv">rip</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">rax</span><span class="p">,</span> <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="nv">.LC2</span><span class="p">[</span><span class="nv">rip</span><span class="p">]</span>
        <span class="nf">movaps</span>  <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="nv">xmm0</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">call</span>    <span class="nv">operator</span> <span class="nv">new</span><span class="p">(</span><span class="nv">unsigned</span> <span class="nv">long</span><span class="p">)</span>
        <span class="nf">movdqa</span>  <span class="nv">xmm5</span><span class="p">,</span> <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">edx</span><span class="p">,</span> <span class="mi">4</span>
        <span class="nf">mov</span>     <span class="nb">rcx</span><span class="p">,</span> <span class="nb">rsp</span>
        <span class="nf">mov</span>     <span class="nb">rbp</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">mov</span>     <span class="nb">edi</span><span class="p">,</span> <span class="nv">OFFSET</span> <span class="nv">FLAT</span><span class="p">:</span><span class="nv">.LC0</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">21</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="mi">3</span>
        <span class="nf">movups</span>  <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rax</span><span class="p">],</span> <span class="nv">xmm5</span>
        <span class="nf">mov</span>     <span class="nb">rax</span><span class="p">,</span> <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="o">+</span><span class="mi">16</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">16</span><span class="p">],</span> <span class="nb">rax</span>
        <span class="nf">call</span>    <span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">vprint</span><span class="p">(</span><span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">basic_string_view</span><span class="o">&lt;</span><span class="nb">ch</span><span class="nv">ar</span><span class="o">&gt;</span><span class="p">,</span> <span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">format_args</span><span class="p">)</span>
        <span class="nf">movdqu</span>  <span class="nv">xmm2</span><span class="p">,</span> <span class="nv">XMMWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">eax</span><span class="p">,</span> <span class="kt">DWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">8</span><span class="p">]</span>
        <span class="nf">mov</span>     <span class="nb">rdi</span><span class="p">,</span> <span class="nb">rbp</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">mov</span>     <span class="nb">edx</span><span class="p">,</span> <span class="kt">DWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span>
        <span class="nf">movq</span>    <span class="nv">xmm1</span><span class="p">,</span> <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rbp</span><span class="o">+</span><span class="mi">16</span><span class="p">]</span>
        <span class="nf">pshufd</span>  <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">xmm2</span><span class="p">,</span> <span class="mi">255</span>
        <span class="nf">movd</span>    <span class="nb">ecx</span><span class="p">,</span> <span class="nv">xmm0</span>
        <span class="nf">pshufd</span>  <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">xmm2</span><span class="p">,</span> <span class="mi">85</span>
        <span class="nf">add</span>     <span class="nb">edx</span><span class="p">,</span> <span class="nb">eax</span>
        <span class="nf">movd</span>    <span class="nb">eax</span><span class="p">,</span> <span class="nv">xmm0</span>
        <span class="nf">movd</span>    <span class="nv">xmm0</span><span class="p">,</span> <span class="nb">edx</span>
        <span class="nf">add</span>     <span class="nb">ecx</span><span class="p">,</span> <span class="nb">eax</span>
        <span class="nf">movd</span>    <span class="nv">xmm4</span><span class="p">,</span> <span class="nb">ecx</span>
        <span class="nf">punpckldq</span>       <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">xmm4</span>
        <span class="nf">paddd</span>   <span class="nv">xmm0</span><span class="p">,</span> <span class="nv">xmm1</span>
        <span class="nf">movq</span>    <span class="nb">rbx</span><span class="p">,</span> <span class="nv">xmm0</span>
        <span class="nf">call</span>    <span class="nv">operator</span> <span class="nv">delete</span><span class="p">(</span><span class="nv">void</span><span class="o">*</span><span class="p">,</span> <span class="nv">unsigned</span> <span class="nv">long</span><span class="p">)</span>
        <span class="nf">add</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">32</span>
        <span class="nf">mov</span>     <span class="nb">rax</span><span class="p">,</span> <span class="nb">rbx</span>
        <span class="nf">pop</span>     <span class="nb">rbx</span>
        <span class="nf">pop</span>     <span class="nb">rbp</span>
        <span class="nf">pop</span>     <span class="nv">r12</span>
        <span class="nf">ret</span>
        <span class="nf">mov</span>     <span class="nv">r12</span><span class="p">,</span> <span class="nb">rax</span>
        <span class="nf">jmp</span>     <span class="nv">.L15</span></code></pre></figure> <p>Well, <code class="language-plaintext highlighter-rouge">Process</code> was inlined into <code class="language-plaintext highlighter-rouge">UseVector</code>, but we still have heap allocation and lots of computation. Compare this to the other <code class="language-plaintext highlighter-rouge">UseStdArray</code> and <code class="language-plaintext highlighter-rouge">UseCArray</code> (again which compile to the same code):</p> <figure class="highlight"><pre><code class="language-nasm" data-lang="nasm"><span class="nf">UseStdArray</span><span class="p">():</span> <span class="c1">; or, UseCArray():</span>
        <span class="nf">sub</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">mov</span>     <span class="nb">edx</span><span class="p">,</span> <span class="mi">4</span>
        <span class="nf">mov</span>     <span class="nb">edi</span><span class="p">,</span> <span class="nv">OFFSET</span> <span class="nv">FLAT</span><span class="p">:</span><span class="nv">.LC0</span>
        <span class="nf">mov</span>     <span class="nb">esi</span><span class="p">,</span> <span class="mi">21</span>
        <span class="nf">mov</span>     <span class="nb">rcx</span><span class="p">,</span> <span class="nb">rsp</span>
        <span class="nf">mov</span>     <span class="kt">QWORD</span> <span class="nv">PTR</span> <span class="p">[</span><span class="nb">rsp</span><span class="p">],</span> <span class="mi">3</span>
        <span class="nf">call</span>    <span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">vprint</span><span class="p">(</span><span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">basic_string_view</span><span class="o">&lt;</span><span class="nb">ch</span><span class="nv">ar</span><span class="o">&gt;</span><span class="p">,</span> <span class="nv">fmt</span><span class="p">::</span><span class="nv">v7</span><span class="p">::</span><span class="nv">format_args</span><span class="p">)</span>
        <span class="nf">add</span>     <span class="nb">rsp</span><span class="p">,</span> <span class="mi">24</span>
        <span class="nf">movabs</span>  <span class="nb">rax</span><span class="p">,</span> <span class="mi">51539607561</span>
        <span class="nf">ret</span></code></pre></figure> <p>This looks much nicer. Now the code just passes the size of the array to the <code class="language-plaintext highlighter-rouge">print</code> function and returns the result, which was computed at compile time! The compiler can “see” through the initialization and remove all of the code for the <code class="language-plaintext highlighter-rouge">Process</code> function.</p> <h2 id="the-real-heroes">The real heroes</h2> <p>So like Samwise in Lord of the Rings, the real hero of this story is <code class="language-plaintext highlighter-rouge">std::span</code>, which allows us to use the simplicity and expressiveness of C arrays in a safe way.</p> <p>Oh, and of course your local neighborhood C++ compiler author (compilers are pretty amazing).</p> <h2 id="edits">Edits</h2> <p>Reddit commenter elcapitaine helpfully <a href="https://www.reddit.com/r/cpp/comments/sc152g/comment/hu4xano/?utm_source=share&amp;utm_medium=web2x&amp;context=3A">pointed out</a> that in C++20 there is a <a href="https://en.cppreference.com/w/cpp/container/array/to_array"><code class="language-plaintext highlighter-rouge">std::to_array</code></a> helper that makes the <code class="language-plaintext highlighter-rouge">UseStdArray</code> case nicer to write:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">Point</span> <span class="nf">UseStdArray</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">input</span> <span class="o">=</span> <span class="n">to_array</span><span class="o">&lt;</span><span class="n">Point</span><span class="o">&gt;</span><span class="p">({{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">},</span> <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}});</span>
  <span class="k">return</span> <span class="nf">Process</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure> <p>This looks much nicer than my code above - it does not list the array size twice and avoids the odd double curly braces.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I just love std::span! I’ve written about it before here and here. Starting in C++20 our friend span lets us write expressive code with little or no cost in standard C++.]]></summary></entry><entry><title type="html">Game theory fun in the NFL</title><link href="https://joshpeterson.github.io/blog/2022/game-theory-fun-in-the-nfl/" rel="alternate" type="text/html" title="Game theory fun in the NFL"/><published>2022-01-13T00:00:00+00:00</published><updated>2022-01-13T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2022/game-theory-fun-in-the-nfl</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2022/game-theory-fun-in-the-nfl/"><![CDATA[<p>Sunday night’s NFL game featured a fun game theory twist. The Los Angeles Chargers and the Las Vegas Raiders would each make the playoffs if they won the game, but they would <em>both</em> make the playoffs if they tied. Of course they could have chosen to kneel down each play of the game, tie 0-0 and be guaranteed a spot in the playoffs.</p> <p>That did not happen - we’ll see why that is not surprising in a moment. The players and coaches made hundreds of decisions throughout the game, but at the pivotal moment, did they make the right decision?</p> <p>To get a handle on what happened in the game, I recommend Bill Barnwell’s great <a href="https://www.espn.com/nfl/story/_/id/33032641/raiders-win-make-nfl-playoffs-did-chargers-blow-teams-played-tie-wild-ending-explained">summary</a> on ESPN.</p> <h2 id="the-prisoners-dilemma">The prisoner’s dilemma</h2> <p>Before the game started the two teams were faced with the class game theory problem called <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma">The Prisoner’s Dilemma</a> (actually it is not <em>quite</em> the same, but we’ll fudge the numbers a bit to make it fit). Basically, both teams could benefit if they can agree and trust each other to keep the agreement (spoiler alert: they can’t).</p> <h2 id="optimization-functions">Optimization functions</h2> <p>We can define <a href="/a-brief-introduction-to-nash-games">Nash games</a> like this in terms of an optimization function for each player, usually named \(J\). In this case, the “players” are the Raiders and Chargers, so we’ll name the functions \(J_R\) and \(J_C\).</p> <p>Each team can choose to play for the win or the tie, and the result of the function will be their seed in the playoffs, either 6 or 7 (spots in the playoffs) or 8 (out of the playoffs). Each team is aiming to get the lowest value for their function.</p> <p>For example, if the Raiders play for a win and the Chargers play for a tie, then the Raiders will win the game and have the 6 seed in the playoffs. Their function looks like this:</p> \[J_R(Win,Tie)=6\] <p>If on the other hand the Raiders play it safe and kneel down each time to tie, but the Chargers play for the win, the Raiders’ function is</p> \[J_R(Tie,Win)=8\] <p>and the Raiders are out of the playoffs. We’ll list the Raiders’ strategy as the first parmeter to the function and the Chargers’ as the second.</p> <p>Each team has four possble outcomes then. For the Raiders they are:</p> \[J_R(Win,Win)=6\] \[J_R(Win,Tie)=6\] \[J_R(Tie,Win)=8\] \[J_R(Tie,Tie)=7\] <p>And for the Chargers:</p> \[J_C(Win,Win)=6\] \[J_C(Win,Tie)=8\] \[J_C(Tie,Win)=6\] \[J_C(Tie,Tie)=6\] <p>One note here: if both teams try to win, only one actually will. So they both plan to get a 6 seed if they win.</p> <p>These are much easier to visualize in a table:</p> \[\begin{array}{c|ccc} &amp; &amp; Chargers \\ \hline &amp; &amp; Win &amp; Tie \\ Raiders &amp; Win &amp; 6,6 &amp; 6,8 \\ &amp; Tie &amp; 8,6 &amp; 7,6 \end{array}\] <p>The first entry in each pair of numbers is the Raiders’ playoff seed (i.e. function value). The second entry is the Chargers playoff seed.</p> <p>We can see very quickly when neither team has an advantage at the start of the game, if one team starts to play for a tie, by, say, kneeling the ball on their first possession, then the other team immediately has an incentive to play to win. So very quickly both teams will play to win, and one of them will not make the playoffs.</p> <h2 id="reducing-the-game">Reducing the game</h2> <p>We can reduce Nash games like this to be <em>minimal ordinal</em> games, where we rank the preferences of each player and ignore the actual optimization function values. This game reduces to this (assuming the Chargers would rather win than tie, even though they get the 6 seed in the playoffs either way):</p> \[\begin{array}{c|ccc} &amp; &amp; Chargers \\ \hline &amp; &amp; Win &amp; Tie \\ Raiders &amp; Win &amp; 1,1 &amp; 1,2 \\ &amp; Tie &amp; 2,1 &amp; 2,2 \end{array}\] <p>By reducing this game to its minimal form, we can care only about the 1’s in the table, and can compare it with other 2x2 games that have the same minimal form. The prisoner’s dilemma game (and many others) reduce to this same minimal ordinal game.</p> <p>The <em>Nash Equilibrium</em> for the game is the cell where two 1’s exist. The predicts the decisions that the players will make, where the game will settle so that no team is changing there decision. And indeed it did! The teams will not collude to tie and both make the playoffs.</p> <h2 id="the-pivotal-play">The pivotal play</h2> <p>But the <em>actual</em> football game brought even more game theory fun! After 69 minutes of football, the teams were tied with just 38 seconds left in overtime.</p> <p>The Raiders had the ball at the Chargers’ 39 yard line. From here they could kick a 57-yard field goal to win - by no means a sure thing. But they were nearly guaranteed a tie, and a trip to the playoffs. Would they kneel the ball to run out the clock, or are they willing to go for the win?</p> <p>At this point both teams again have two options. The Raiders can try play for the win and try to move the ball closer for an easier field goal, or they can kneel down and end the game. The Chargers can stick with their base defense, or call a time out to put in a run-specific defense to try to stop the Raiders from getting closer.</p> <p>The game looks like this, again expressed in terms of playoff seeds:</p> \[\begin{array}{c|ccc} &amp; &amp; Chargers \\ \hline &amp; &amp; Play &amp; Change \\ Raiders &amp; Run &amp; 6,8 &amp; 6,6 \\ &amp; Kneel &amp; 7,6 &amp; 7,6 \end{array}\] <p>Let’s reduce this one a well:</p> \[\begin{array}{c|ccc} &amp; &amp; Chargers \\ \hline &amp; &amp; Play &amp; Change \\ Raiders &amp; Run &amp; 1,2 &amp; 1,1 \\ &amp; Kneel &amp; 2,1 &amp; 2,1 \end{array}\] <p>The Chargers clearly want the Raiders to kneel down, then no matter what they do, the Chargers make the playoffs. But if the Raiders kneel, the will get the 7 seed, and that is pretty much guaranteed at this point. So the Raiders can take the initiative and go for that 6 seed.</p> <p>The game has changed now from a Nash game, where the players are balanced, to a <a href="https://en.wikipedia.org/wiki/Stackelberg_competition">Stackelberg</a> game, where one player is the leader, who acts first.</p> <p>The Raiders are the leader here, and there best option is the execute a play (likely a low-risk running play, to keep the clock going). Sure enough, they did not line up in a formation to kneel the ball - they were ready to try to go for the win. So we forget about the second row in this table - the Raiders will execute a play. What will the Chargers do?</p> <p>The Chargers recognized that their current defense did not have the correct personnel on the field to stop a running play, so their coach called a time out in order to change the personnel.</p> <p>At first this seems wrong - why would the Chargers’ coach call a timeout and stop the clock when he wants the game to end in a tie? But recognizing the Raiders incentive to execute a play and try to gain enough yards to kick a field goal and win, the Chargers’ coach made the correct decision.</p> <p>In the end, the Raiders ran a play against the Chargers better run defense - and still gained 10 yards! So although the Chargers made the correct decision, the execution of the play changed the game again. Now the Raiders can (and did) take a high-percentage field goal and made it to win the game.</p> <h2 id="a-counter-factual">A counter-factual</h2> <p>What would have happened if the Chargers had the ball in the same situation? The game would have looked like this:</p> \[\begin{array}{c|ccc} &amp; &amp; Chargers \\ \hline &amp; &amp; Run &amp; Kneel \\ Raiders &amp; Play &amp; 7,6 &amp; 7,6 \\ &amp; Change &amp; 7,6 &amp; 7,6 \end{array}\] <p>The Chargers could not improve their playoff seeding by running plays to try to get closer to kick a winning field goal. The Raiders also have no options to improve their seeding. So they likely would have kneeled the ball, and both team would have made the playoffs. In a Stackelberg game, being the leader really matters!</p> <h2 id="game-theory-ftw">Game theory FTW!</h2> <p>While I doubt the coaches or players were actively doing this kind of analysis before or during the game, I find it fascinating that game theory correctly predicted the behavior of the teams both teams.</p> <p>I wonder what else game theory can predict?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Sunday night’s NFL game featured a fun game theory twist. The Los Angeles Chargers and the Las Vegas Raiders would each make the playoffs if they won the game, but they would both make the playoffs if they tied. Of course they could have chosen to kneel down each play of the game, tie 0-0 and be guaranteed a spot in the playoffs.]]></summary></entry><entry><title type="html">Docker for C++ builds</title><link href="https://joshpeterson.github.io/blog/2020/docker-for-c-plus-plus-builds/" rel="alternate" type="text/html" title="Docker for C++ builds"/><published>2020-06-26T00:00:00+00:00</published><updated>2020-06-26T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2020/docker-for-c-plus-plus-builds</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2020/docker-for-c-plus-plus-builds/"><![CDATA[<p>A few years ago, one of my colleagues at Unity was discussing some work to get our tests running on a new local CI system. She described a recent discussion with developer services team as “Docker, Docker, Kubernetes” (hearkening back to the famous Seinfeld <a href="https://en.wikipedia.org/wiki/The_Yada_Yada">Yada, Yada</a> episode). As a developer with no experience in “cloud” things, this is often how I feel. These “containers” seem to be so useful, yet magical and out of reach for me. So I decided to go off on a quest and slay the “Docker, Docker, Kubernetes” dragon (well, at least Docker, I still don’t know what Kubernetes is).</p> <h2 id="using-docker-on-ci-for-c-linux-builds">Using Docker on CI for C++ Linux builds</h2> <p>One of my biggest difficulties with hobby C++ project is dealing with CI systems. <a href="https://travis-ci.com">Travis CI</a> has a great free service for open source projects, but often I would complete a new feature locally, only to see it fail on CI. There must be a better way to reproduce the CI build configuration locally!</p> <p>In addition, keeping up with the latest C++ compiler versions on CI is difficult. The feedback loop required to change a .yml file, push a change, then wait for a build to run on CI is simply too long. Local iteration would be much more efficient.</p> <h3 id="the-final-product">The final product</h3> <p>If you’re not interested in the details, just check out the Travis CI <a href="https://github.com/joshpeterson/cpp-template/blob/master/.travis.yml">configuration</a> in my cpp-template repository. I’m pretty happy with the end result. Each CI step on Linux is a single <code class="language-plaintext highlighter-rouge">docker</code> invocation that I can easily run locally.</p> <h3 id="creating-a-docker-container">Creating a Docker container</h3> <p>A container is a lightweight Linux installation where you explicitly define <em>everything</em> that is installed.</p> <ul> <li>Lightweight: it is much smaller in size than a full Linux installation; it “boots up” very fast.</li> <li>Linux: It is running a real Linux OS, and must run “on top of” a full Linux installation.</li> <li>You define what is installed: You must indicate all of the packages (e.g. in the apt-get sense) that are installed - you don’t have access to anything else.</li> </ul> <p>I’m sure this is not a good technical description of containers, but for someone with no experience in this world, I find it helpful to view them like this.</p> <p><a href="https://docker.com">Docker</a> is a tool for creating and running containers (there are probably other such tools out there). It accepts a Dockerfile as input. A Dockerfile is a text document written in a domain-specific language that Docker understands. This file is used to tell Docker about that third point above - what packages should be installed on your Linux container.</p> <p>These Docker inputs can get pretty complex, but you can also accomplish a lot with a simple one. My Dockerfile for builds with GCC is below:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FROM gcc:10.1.0
RUN apt-get update
RUN apt-get install -y cmake ninja-build time
</code></pre></div></div> <p>Let’s break this down:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FROM gcc:10.1.0
</code></pre></div></div> <p>The first line is the most interesting. The <code class="language-plaintext highlighter-rouge">FROM</code> statement means “Create this container by starting from another container first.” The starting container in this case is named “gcc” (more on that later). The part after the colon is the “tag”, which could be anything, but is most often a version number.</p> <p>But where does this “gcc” container come from? Docker has a service called <a href="https://hub.docker.com/">Dockerhub</a> where anyone can publish Docker containers. The name “gcc” means “Look for a container named gcc on Dockerhub. If you find one, download it, and use it as the starting point for my container.”</p> <p>The GCC developers publish a container on Dockerhub for each GCC release. This container is based in turn on a Debian Linux container. So now my container can have everything set up and ready to go to use GCC version 10.1.0 with just one line in my Dockerfile, neat!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RUN apt-get update
RUN apt-get install -y cmake ninja-build time
</code></pre></div></div> <p>These last to lines use the <code class="language-plaintext highlighter-rouge">RUN</code> statement, which tells Docker to execute the text following is as a shell command. The two specific commands I’m using tell the Debian package manager (<code class="language-plaintext highlighter-rouge">apt</code>) to first update to repositories to look for the latest packages, then install three packages that I need to build the C++ code in my projects: CMake, Ninja, and the <code class="language-plaintext highlighter-rouge">time</code> command (see, you really need to install, <em>everything</em> you need).</p> <h3 id="solving-my-problems">Solving my problems</h3> <p>Wow, so this is really cool - I’m doing cloud stuff! ☁️ But seriously, this solves my two problems with CI:</p> <ul> <li>Updating the version of a C++ compiler is now as simple as changing one version number in my Dockerfile.</li> <li>I can test my CI set up locally by running one Docker command, so iteration on CI changes is now much faster.</li> </ul> <p>🎉</p> <h3 id="my-docker-images">My docker images</h3> <p>I’ve created Docker images for <a href="https://hub.docker.com/r/petersonjm1/gcc">GCC</a>, <a href="https://hub.docker.com/r/petersonjm1/clang">Clang</a>, and <a href="https://hub.docker.com/r/petersonjm1/emscripten">Emscripten</a> on Linux (I’m working on a Windows image, but that is not complete yet). You can check them out on Dockerhub and maybe use them as a base for your image. Happy cloud fun! 🌩️</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A few years ago, one of my colleagues at Unity was discussing some work to get our tests running on a new local CI system. She described a recent discussion with developer services team as “Docker, Docker, Kubernetes” (hearkening back to the famous Seinfeld Yada, Yada episode). As a developer with no experience in “cloud” things, this is often how I feel. These “containers” seem to be so useful, yet magical and out of reach for me. So I decided to go off on a quest and slay the “Docker, Docker, Kubernetes” dragon (well, at least Docker, I still don’t know what Kubernetes is).]]></summary></entry><entry><title type="html">A zero cost abstraction?</title><link href="https://joshpeterson.github.io/blog/2018/a-zero-cost-abstraction/" rel="alternate" type="text/html" title="A zero cost abstraction?"/><published>2018-11-21T00:00:00+00:00</published><updated>2018-11-21T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2018/a-zero-cost-abstraction</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2018/a-zero-cost-abstraction/"><![CDATA[<p>Recently Joachim (CTO at Unity) has been talking about “performance by default”, the mantra that software should be as fast as possible from the outset. This is driving the pretty cool stuff many at Unity are doing around things like ECS, the C# job system, and Burst (find lots more about that <a href="https://unity3d.com/unity/features/job-system-ECS">here</a>).</p> <p>One question Joachim has asked internally of Unity developers is (I’m paraphrasing here): “What is the absolute lower bound of time this code could use?” This strikes me as a really useful way to think about performance. The question changes from “How fast is this?” to “How fast could this be?”. If the answers to those two questions are not the same, the next question is “Do we <em>really</em> need the additional overhead?”</p> <p>Another way to think about this is to consider the zero-cost abstraction, a concept much discussed in the C++ and Rust communities. Programmers are always building abstractions, and those abstractions often lead to the difference between “how fast it is” and “how fast it could be”. We want to provide useful abstractions that don’t hurt performance.</p> <h2 id="reading-some-bytes">Reading some bytes</h2> <p>I was thinking about all of this recently while writing some code to read bytes from a binary file. The first bit of code that rolled off my fingers was:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">uint8_t</span> <span class="nf">ReadByte</span><span class="p">();</span>

<span class="kt">void</span> <span class="nf">ReadBytes</span><span class="p">(</span><span class="kt">uint8_t</span><span class="o">*</span> <span class="n">buffer</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">size</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
      <span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReadByte</span><span class="p">();</span>
<span class="p">}</span></code></pre></figure> <p>This feels like the “canonical” way to read bytes into a buffer. The API has no abstraction - the function gets exactly what it needs: a pointer to some memory location and the number of bytes to read into that memory location.</p> <p>I ran clang-tidy on this code, and it was not happy:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>binary_reader.cpp:20:5: error: do not use pointer arithmetic
[cppcoreguidelines-pro-bounds-pointer-arithmetic,-warnings-as-errors]
    buffer[i] = ReadByte();
</code></pre></div></div> <p>At first this error was a bit confusing, but after staring at the code a bit, I think I determined why clang-tidy doesn’t like it: pointer arithmetic, while the fastest way to address a buffer, is prone to errors. Specifically, the user of this function can pass <em>any</em> value for <code class="language-plaintext highlighter-rouge">size</code>. The function has no choice but to dutifully write to memory where the client asked for it, so even a well-meaning client who passes the wrong <code class="language-plaintext highlighter-rouge">size</code> can cause memory corruption. We need an abstraction which makes this function difficult to misuse.</p> <h2 id="enter-the-span">Enter the span</h2> <p>What options do we have for an interface to <code class="language-plaintext highlighter-rouge">ReadBytes</code> that makes it easy to pass the size of the buffer correctly? Lets’ list a few</p> <ol> <li><code class="language-plaintext highlighter-rouge">ReadBytes(std::vector&lt;uint8_t&gt;&amp; buffer);</code></li> <li><code class="language-plaintext highlighter-rouge">template&lt;size_t N&gt; ReadBytes(std::array&lt;uint8_t, N&gt;&amp; buffer);</code></li> <li><code class="language-plaintext highlighter-rouge">template&lt;typename Iterator&gt; ReadBytes(Interator begin, Iterator end);</code></li> </ol> <p>All of these will work, but they seem a bit restrictive of the client in different ways. In addition, some of these APIs encode <em>more</em> information than we really need. All the client really wants to say is: “Here is a buffer I’ve set aside in memory, please fill it up with bytes, thanks!”</p> <p>Thankfully, there is an abstraction in C++20 for a collection of objects of a given type, along with their size: <a href="https://en.cppreference.com/w/cpp/container/span"><code class="language-plaintext highlighter-rouge">std::span</code></a>. I’m not using C++20 for this project, but I can use the same type from the GSL, <a href="https://github.com/Microsoft/GSL/blob/master/include/gsl/span"><code class="language-plaintext highlighter-rouge">gsl::span</code></a>. I wrote a bit about span’s cousin <code class="language-plaintext highlighter-rouge">gsl::multi_span</code> <a href="/using-span-with-argv">earlier</a>, learning that it has a small, non-zero cost. I wanted to dive a bit deeper with span.</p> <p>The new implementation of <code class="language-plaintext highlighter-rouge">ReadBytes</code> looks like this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="nf">ReadBytes</span><span class="p">(</span><span class="n">gsl</span><span class="o">::</span><span class="n">span</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span> <span class="n">buffer</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">value</span> <span class="o">:</span> <span class="n">buffer</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ReadByte</span><span class="p">();</span>
<span class="p">}</span></code></pre></figure> <p>Better yet, I can call it with a number of different buffer types, all in a simple way:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="p">,</span><span class="mi">8</span><span class="o">&gt;</span> <span class="n">buffer</span><span class="p">;</span>
<span class="n">ReadBytes</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span> <span class="n">buffer</span><span class="p">(</span><span class="mi">8</span><span class="p">);</span>
<span class="n">ReadBytes</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>

<span class="kt">uint8_t</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
<span class="n">ReadBytes</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span></code></pre></figure> <p>For each of these cases, the compiler infers the size of the buffer, so I don’t have to make sure I pass the proper size of the buffer. Problem solved!</p> <h2 id="but-at-what-cost">But at what cost?</h2> <p>Now we have a safe API to read bytes into a buffer, and we know the baseline fastest way to read bytes into the buffer. If performance by default matters, we need to know how much we pay for this safe abstraction.</p> <p>One good way to understand the cost of code is to investigate the generated assembly code, where there is <em>very</em> little abstraction. Take a look at this <a href="https://godbolt.org/z/BdEqK5">comparison</a> (the first implementation is on the left, the safe one is on the right).</p> <p>The safe implementation only costs us one additional comparison instruction (line 15 on the right) at the start of the for loop. I think this is checking an error condition, so I expect most of the time this branch will not be taken. I assume the processor will notice that pretty quickly and optimize for the non-error case.</p> <p>Note that we do pay a code size price for this abstraction as well. We have five additional instructions here.</p> <p>Lets measure the run time performance cost. It looks like it will be small, but can we be sure? I wrote this benchmark:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="cp">#include</span> <span class="cpf">"binary_reader.h"</span><span class="cp">
#include</span> <span class="cpf">&lt;benchmark/benchmark.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">ReadEightBytesRaw</span><span class="p">(</span><span class="n">benchmark</span><span class="o">::</span><span class="n">State</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">BinaryReader</span> <span class="n">reader</span><span class="p">(</span><span class="s">"../../../test/data/simple.wasm"</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">_</span> <span class="o">:</span> <span class="n">state</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="kt">uint8_t</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
    <span class="n">reader</span><span class="p">.</span><span class="n">ReadBytes</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="mi">8</span><span class="p">);</span>
    <span class="n">benchmark</span><span class="o">::</span><span class="n">DoNotOptimize</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
    <span class="n">reader</span><span class="p">.</span><span class="n">Reset</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">BENCHMARK</span><span class="p">(</span><span class="n">ReadEightBytesRaw</span><span class="p">);</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">ReadEightBytesSpan</span><span class="p">(</span><span class="n">benchmark</span><span class="o">::</span><span class="n">State</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">BinaryReader</span> <span class="n">reader</span><span class="p">(</span><span class="s">"../../../test/data/simple.wasm"</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">_</span> <span class="o">:</span> <span class="n">state</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="kt">uint8_t</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
    <span class="n">reader</span><span class="p">.</span><span class="n">ReadBytesSpan</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
    <span class="n">benchmark</span><span class="o">::</span><span class="n">DoNotOptimize</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
    <span class="n">reader</span><span class="p">.</span><span class="n">Reset</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">BENCHMARK</span><span class="p">(</span><span class="n">ReadEightBytesSpan</span><span class="p">);</span></code></pre></figure> <p>And here are the results:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running ./bench
Run on (8 X 2693.7 MHz CPU s)
CPU Caches:
L1 Data 32K (x4)
L1 Instruction 32K (x4)
L2 Unified 1024K (x4)
L3 Unified 33792K (x4)
Load Average: 0.08, 0.15, 0.13
----------------------------------------------------------
Benchmark                   Time           CPU Iterations
----------------------------------------------------------
ReadEightBytesRaw         365 ns        365 ns    1918628
ReadEightBytesSpan        364 ns        364 ns    1921003
</code></pre></div></div> <p>Although much of the time in the benchmark is used opening and seeking in the binary file, the profiler indicates the percent of time spent in the byte reading code is nearly identical, with maybe slight advantage to the raw version, which might not even be outside the margin of error.</p> <p>So using <code class="language-plaintext highlighter-rouge">span</code> as an abstraction for an arbitrary length buffer to receive data from a function provides:</p> <ol> <li>A safe interface</li> <li>A flexible interface</li> <li>No cost over the best performing case</li> </ol> <p>In this case, <code class="language-plaintext highlighter-rouge">span</code> gives us the API we want with performance by default.</p> <h2 id="update-november-24">Update (November 24)</h2> <p>Reddit user <a href="https://www.reddit.com/user/TheThiefMaster">u/TheThiefMaster</a> points out that GCC will <a href="https://godbolt.org/z/UrDA07">optimize</a> the <code class="language-plaintext highlighter-rouge">span</code> case so that the code is the same as the unsafe case, meaning it really has no overhead.</p> <p><em>Discuss this post on Reddit <a href="https://www.reddit.com/r/cpp/comments/9z2hqq/a_zero_cost_abstraction/">here</a>.</em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Recently Joachim (CTO at Unity) has been talking about “performance by default”, the mantra that software should be as fast as possible from the outset. This is driving the pretty cool stuff many at Unity are doing around things like ECS, the C# job system, and Burst (find lots more about that here).]]></summary></entry><entry><title type="html">A C++ Template Project</title><link href="https://joshpeterson.github.io/blog/2018/a-cpp-template-project/" rel="alternate" type="text/html" title="A C++ Template Project"/><published>2018-09-26T00:00:00+00:00</published><updated>2018-09-26T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2018/a-cpp-template-project</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2018/a-cpp-template-project/"><![CDATA[<p>How should I get started with a new C++ project? Usually when I have an idea for a new project, this is the first question I need to answer. Many other languages have a simple answer, like <code class="language-plaintext highlighter-rouge">cargo new hello_world --bin</code> (Rust) or <code class="language-plaintext highlighter-rouge">rails new blog</code> (Ruby on Rails). In just a few seconds, I’m writing the actual code for the project, without the need to worry about the minutiae of setting up a build or unit test system or even continuous integration.</p> <p>Don’t get me wrong, I love the flexibility of C++, but without a common project system, getting starting can be daunting. This time, I decided to start a new project with a few days of setup, then use that setup as a general template project. More than a month later, here we are! But I do have a <a href="https://github.com/joshpeterson/cpp-template">C++ template project</a> which has proven useful for me. I hope it will be for you also.</p> <h2 id="roots">Roots</h2> <p>This C++ template project is based on the project from the excellent <a href="https://arne-mertz.de/2018/05/hello-cmake/">Hello, CMake!</a> blog post series by Arne Mertz. Most often, I struggle early in the project with indecision about the directory structure. Should production and test code be in separate directories? How does test code access production code? Should the build output be in-tree or out-of-tree? Arne’s project structure answers these question with simplicity in ways that make sense to me.</p> <h2 id="whats-in-the-box">What’s in the box</h2> <p>The template project comes with some opinionated choices. They work for me - but they are easy to change if something works better for you. The project builds with <a href="https://cmake.org/">CMake</a> and uses <a href="ihttps://github.com/catchorg/Catch2">Catch</a> for unit tests. It builds on Linux and macOS via Travis CI and on Windows via Appveyor.</p> <h3 id="project-structure">Project structure</h3> <p>The project has the following top-level directories:</p> <ul> <li>The src directory is the location of all of the project’s source code (header files and source files). The main.cpp file is built into the final executable, all other source files in this directory are built into a static library. Only code in this static library will be tested.</li> <li>The test directory contains the unit tests. The unit test executable links with the static library built from src directory.</li> <li>The thirdparty directory contains external code used by this project, namely, Catch and the CMake sanitizer integration.</li> <li>The tools directory contains a number of scripts used for building and other tool integration with the project.</li> </ul> <h3 id="other-tool-integration">Other tool integration</h3> <p>The project integrates with a few other tools to aid in development.</p> <ul> <li>The clang-format utility is used to enforce common source code formatting. The tools/format script can be used locally to update code formatting to match the style in the .clang-format file. The tools/run-clang-format.py script is used on Travis CI to check formatting.</li> <li>The clang-tidy utility is used to run static analysis on the source code. The tools/tidy script can be used locally and on Travis CI to run clang-tidy.</li> <li>The clang address, thread, and undefined behavior sanitizers are run on the unit tests. The tools/sanitize script can be used to run them locally.</li> </ul> <p>Since I primarily do hobby development on Linux (<a href="https://joshpeterson.github.io/hobby-development-on-azure">on Azure!</a>), these tooling scripts are configured to run on Linux.</p> <h3 id="my-favorite-tool">My favorite tool</h3> <p>One of my favorite tools in the project is the very simple <a href="https://github.com/joshpeterson/cpp-template/blob/master/tools/watch"><code class="language-plaintext highlighter-rouge">watch</code></a> script. I enjoy running unit tests each time I save a source file. I’ve used many-featured <a href="https://github.com/guard/guard">Guard</a> project for this before. It works great on Ruby projects where I already use Bundler, but the requirement to have Bundler installed for a C++ project is pretty high. Enter the wonderful little <a href="http://www.entrproject.org/"><code class="language-plaintext highlighter-rouge">entr</code></a> tool, which gives me the immediate feedback I want without the need for Guard and Bundler.</p> <h2 id="try-it-for-yourself">Try it for yourself</h2> <p>Most of all, I enjoy how commands like <code class="language-plaintext highlighter-rouge">cargo new</code> or <code class="language-plaintext highlighter-rouge">rails new</code> quickly get a working project up and running, immediately ready for real code. While not as pithy, this command will get you started with my C++ template project:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl
https://raw.githubusercontent.com/joshpeterson/cpp-template/master/cpp-template-installer.py
| python - &lt;my project name&gt;
</code></pre></div></div> <p>So give it a try, modify the template to meet your needs, and start building someting fun!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How should I get started with a new C++ project? Usually when I have an idea for a new project, this is the first question I need to answer. Many other languages have a simple answer, like cargo new hello_world --bin (Rust) or rails new blog (Ruby on Rails). In just a few seconds, I’m writing the actual code for the project, without the need to worry about the minutiae of setting up a build or unit test system or even continuous integration.]]></summary></entry><entry><title type="html">Hobby Development on Azure</title><link href="https://joshpeterson.github.io/blog/2018/hobby-development-on-azure/" rel="alternate" type="text/html" title="Hobby Development on Azure"/><published>2018-05-30T00:00:00+00:00</published><updated>2018-05-30T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2018/hobby-development-on-azure</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2018/hobby-development-on-azure/"><![CDATA[<p>For the past few months, I’ve tried to move all of my hobby development activities to a VM on Microsoft Azure. The results have been pretty promising. MSDN members receive free Azure credits each month. This benefit is more than enough to cover the costs for my development needs.</p> <h1 id="the-costs">The costs</h1> <p>When I first started to try this, I was very confused by the cost structure. Most of the information available seems to be aimed at use of Azure for servers, so everything is focused on hourly costs for machines running twenty-four hours per day. Thankfully, there is information applicable to more piecemeal usage, like hobby development.</p> <h2 id="virtual-machine-cost">Virtual machine cost</h2> <p>Azure offers three ways to pay for a VM - Pay As You Go, 1 Year Reserved, and 3 Years Reserved. Since I’m not planning to use this VM all of the time, the Pay As You Go option works best for hobby development. I found the VM <a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/">pricing</a> to make sense.</p> <p>For my use case, I wanted VMs that with SSD drives for the OS disk. This was a bit difficult to find initially, but eventually I settled on the Fv2 Series running Ubuntu Linux. Currently, these VMs are only available in the West US 2 region. If you don’t care about the region of the VM, check each of the available regions - there may be different VM options in each region.</p> <h2 id="disk-cost">Disk cost</h2> <p>I was rather confused by disk cost for a while. The VMs have an OS disk, but is the data on the disk persistent? Or do I need to also pay for storage? I don’t need a database service like a website might, I just want a normal disk which can store data over time.</p> <p>It turns out that each VM can be configured with a different OS disk - with a certain size and performance characteristics. This disk is persistent, so I can set up the OS and store in-progress work on it as I would on a local computer. In Azure, these are called “managed disks”, their <a href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/">pricing</a> is again organized by region and type.</p> <p>I wanted the performance of an SSD, so I’m using a Premium Managed Disk in the West US 2 region. Hard disk drives are called Standard Managed Disks. Again, options may vary by region.</p> <h1 id="how-it-all-works">How it all works</h1> <p>Putting this all together, I’m using the following:</p> <ol> <li><a href="https://azure.microsoft.com/en-us/pricing/member-offers/credit-for-visual-studio-subscribers/">Free MSDN account</a>, with $50/month Azure credit</li> <li>256 GB Premium Managed Disk (SSD) - $34.56/month</li> <li>Ubuntu VM, F8v2 - $0.358/hour</li> </ol> <p>This configuration lets me use an 8 core machine with 16 GB of RAM. After the OS disk cost, I have $15.44 to spend each month on the VM. That leaves me with about 43 hours per month of time the F8v2 VM can be running. Azure let’s me easily re-size the VM, so I can switch to a smaller, less expensive one if I start to run out of credit.</p> <p>My schedule allows me to spend 3-4 hours on hobby development per week at most, so even with some wiggle room to let the VM run for a few hours to complete builds, I’m well under the total monthly credit.</p> <h2 id="shutting-down-the-vm">Shutting down the VM</h2> <p>The key to make all of this work out is shutting down the VM so it is not incurring costs when not in use. Since I’m the only user on this VM, I don’t need to leave it running like I would a server. Note that the VM must be shutdown via the Azure portal to make this work. Shutting it down from the Linux command line is not enough, since Azure will keep its resources in use.</p> <p>Each time the VM is started, it will get new resources, including a new IP address. However, the VM can be assigned a host name, so the difference in IP address is not important for my use case.</p> <h2 id="going-over-the-limits">Going over the limits</h2> <p>More than once in the last few months I’ve accidentally used too many resources, and went over my monthly budget for the free plan. Thankfully, Azure does not require a form of payment beyond the free plan. Once the credit is exhausted, VMs cannot be started until the credit resets next month. All of the configuration and data on the VM is persisted, so it is easy to start again the next month.</p> <h1 id="a-great-deal">A great deal</h1> <p>Since I’m only using this VM a few hours per week, Azure is the perfect solution for my hobby development needs. A comparable physical machine would have significant up-front cost, and would still not get any more usage than the Azure VM. I’ve tried to do hobby development like this with other cloud providers, but the free plan for Azure offers the most value, by far.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[For the past few months, I’ve tried to move all of my hobby development activities to a VM on Microsoft Azure. The results have been pretty promising. MSDN members receive free Azure credits each month. This benefit is more than enough to cover the costs for my development needs.]]></summary></entry><entry><title type="html">Introducing Struct Layout</title><link href="https://joshpeterson.github.io/blog/2018/introducing-struct-layout/" rel="alternate" type="text/html" title="Introducing Struct Layout"/><published>2018-04-17T00:00:00+00:00</published><updated>2018-04-17T00:00:00+00:00</updated><id>https://joshpeterson.github.io/blog/2018/introducing-struct-layout</id><content type="html" xml:base="https://joshpeterson.github.io/blog/2018/introducing-struct-layout/"><![CDATA[<p>I’ve built a fun little tool to help understand the way a C or C++ compiler will layout members in a struct or class. Behold, <a href="https://structlayout.herokuapp.com">Struct Layout</a>. This tool uses the <a href="https://github.com/joshpeterson/layout">Layout</a> utility behind the scenes. Layout parses C and C++ code using libclang. It generates C code with proper <code class="language-plaintext highlighter-rouge">sizeof</code> and <code class="language-plaintext highlighter-rouge">offsetof</code> operators to output a table including the size of each type, the size, offset, and padding for each field.</p> <p>This information can be useful, as it may be surprising how a compiler lays out a given type in memory. The compiler may need to deal with platform-specific alignment requirements for certain types. If you are writing cross-platform code, it can be useful to understand how compilers behave on all of your target platforms.</p> <p>If you are thinking about data-oriented design, data layout can be rather important. Knowing the number of object which fit into a cache line can inform your decisions about how algorithms can best make use of processor and memory resources.</p> <p>I’ve already found this tool useful for day-to-day work, I hope that you do as well!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’ve built a fun little tool to help understand the way a C or C++ compiler will layout members in a struct or class. Behold, Struct Layout. This tool uses the Layout utility behind the scenes. Layout parses C and C++ code using libclang. It generates C code with proper sizeof and offsetof operators to output a table including the size of each type, the size, offset, and padding for each field.]]></summary></entry></feed>